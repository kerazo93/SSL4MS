{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f2ae721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b607196e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/kevinerazocastillo/Desktop/MS_SSL/SSL4MS/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0b7a87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTORCH_ENABLE_MPS_FALLBACK=1\n"
     ]
    }
   ],
   "source": [
    "%env PYTORCH_ENABLE_MPS_FALLBACK=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b12d100",
   "metadata": {},
   "source": [
    "# Set Up Data Module for Self-Supervised Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69075a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_utils import MassSpecSelfSupervisedDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b4c5b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = MassSpecSelfSupervisedDataModule(root_dir='/Users/kevinerazocastillo/Desktop/MS_SSL/processed_data/spectra/',\n",
    "                                     train_csv='/Users/kevinerazocastillo/Desktop/MS_SSL/processed_data/labels/train_labels_split1.csv',\n",
    "                                     val_csv='/Users/kevinerazocastillo/Desktop/MS_SSL/processed_data/labels/val_labels_split1.csv',\n",
    "                                     test_csv='/Users/kevinerazocastillo/Desktop/MS_SSL/processed_data/labels/test_labels_split1.csv',\n",
    "                                     nl_csv='/Users/kevinerazocastillo/Desktop/MS_SSL/SSL4MS/data/neutral_losses.csv',\n",
    "                                     corrupt_prob=0.1,\n",
    "                                     max_len=128,\n",
    "                                     batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7bee74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a201a3",
   "metadata": {},
   "source": [
    "# Set Up Model Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75b79610",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.selfsupervised import SpectrumSymmetricAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69bd913c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_AE = SpectrumSymmetricAE(num_heads=4, ffn_factor=4, dropout=0.1, hidden_dims=[8, 32, 128], max_len=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "509f5fa8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpectrumSymmetricAE(\n",
      "  (act): Sigmoid()\n",
      "  (encoder): SpectrumEncoder(\n",
      "    (act): Sigmoid()\n",
      "    (enc_list): ModuleList(\n",
      "      (0): EncoderBlock(\n",
      "        (act): Sigmoid()\n",
      "        (pe): PositionalEncoding(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (cnn): Conv1d(2, 8, kernel_size=(1,), stride=(1,), padding=same)\n",
      "        (enc): EncoderLayerGLU(\n",
      "          (act): Sigmoid()\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)\n",
      "          )\n",
      "          (ffn): FFN(\n",
      "            (fc1): Linear(in_features=8, out_features=32, bias=True)\n",
      "            (fc2): Linear(in_features=32, out_features=8, bias=True)\n",
      "            (glu): GLU(\n",
      "              (act): Sigmoid()\n",
      "              (linear1): Linear(in_features=32, out_features=32, bias=True)\n",
      "              (linear2): Linear(in_features=32, out_features=32, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): EncoderBlock(\n",
      "        (act): Sigmoid()\n",
      "        (pe): PositionalEncoding(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (cnn): Conv1d(8, 32, kernel_size=(1,), stride=(1,), padding=same)\n",
      "        (enc): EncoderLayerGLU(\n",
      "          (act): Sigmoid()\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
      "          )\n",
      "          (ffn): FFN(\n",
      "            (fc1): Linear(in_features=32, out_features=128, bias=True)\n",
      "            (fc2): Linear(in_features=128, out_features=32, bias=True)\n",
      "            (glu): GLU(\n",
      "              (act): Sigmoid()\n",
      "              (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): EncoderBlock(\n",
      "        (act): Sigmoid()\n",
      "        (pe): PositionalEncoding(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (cnn): Conv1d(32, 128, kernel_size=(1,), stride=(1,), padding=same)\n",
      "        (enc): EncoderLayerGLU(\n",
      "          (act): Sigmoid()\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (ffn): FFN(\n",
      "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (glu): GLU(\n",
      "              (act): Sigmoid()\n",
      "              (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): SpectrumDecoder(\n",
      "    (act): Sigmoid()\n",
      "    (dec_list): ModuleList(\n",
      "      (0): EncoderBlock(\n",
      "        (act): Sigmoid()\n",
      "        (pe): PositionalEncoding(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (cnn): Conv1d(128, 32, kernel_size=(1,), stride=(1,), padding=same)\n",
      "        (enc): EncoderLayerGLU(\n",
      "          (act): Sigmoid()\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
      "          )\n",
      "          (ffn): FFN(\n",
      "            (fc1): Linear(in_features=32, out_features=128, bias=True)\n",
      "            (fc2): Linear(in_features=128, out_features=32, bias=True)\n",
      "            (glu): GLU(\n",
      "              (act): Sigmoid()\n",
      "              (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): EncoderBlock(\n",
      "        (act): Sigmoid()\n",
      "        (pe): PositionalEncoding(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (cnn): Conv1d(32, 8, kernel_size=(1,), stride=(1,), padding=same)\n",
      "        (enc): EncoderLayerGLU(\n",
      "          (act): Sigmoid()\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)\n",
      "          )\n",
      "          (ffn): FFN(\n",
      "            (fc1): Linear(in_features=8, out_features=32, bias=True)\n",
      "            (fc2): Linear(in_features=32, out_features=8, bias=True)\n",
      "            (glu): GLU(\n",
      "              (act): Sigmoid()\n",
      "              (linear1): Linear(in_features=32, out_features=32, bias=True)\n",
      "              (linear2): Linear(in_features=32, out_features=32, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_cnn): Conv1d(8, 2, kernel_size=(1,), stride=(1,), padding=same)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(spec_AE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296335c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f44de4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffe66d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70b491f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitSpectrumSymmetricAE(L.LightningModule):\n",
    "    def __init__(self, symmAE):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.ae = symmAE\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x_corr, x_true = batch\n",
    "        x_dec, x_enc, x_mask = self.ae(x_corr)\n",
    "        loss = nn.MSELoss()(x_dec, x_true)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x_corr, x_true = batch\n",
    "        x_dec, x_enc, x_mask = self.ae(x_corr)\n",
    "        val_loss = nn.MSELoss()(x_dec, x_true)\n",
    "        self.log(\"val_loss\", val_loss)\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x_corr, x_true = batch\n",
    "        x_dec, x_enc, x_mask = self.ae(x_corr)\n",
    "        test_loss = nn.MSELoss()(x_dec, x_true)\n",
    "        self.log(\"test_loss\", test_loss)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-4, weight_decay=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3b720e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinerazocastillo/anaconda3/envs/cheminf_MS/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'symmAE' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['symmAE'])`.\n"
     ]
    }
   ],
   "source": [
    "lit_AE = LitSpectrumSymmetricAE(spec_AE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df5e3b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d26c8ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", patience=3, verbose=False, mode=\"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5e3b90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60c447fb",
   "metadata": {},
   "source": [
    "# Set Up the Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ced779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StochasticWeightAveraging(swa_lrs=1e-3),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cac58993",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import StochasticWeightAveraging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13971d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "/Users/kevinerazocastillo/anaconda3/envs/cheminf_MS/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py:125: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/kevinerazocastillo/anaconda3/envs/cheminf_MS/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:67: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(gradient_clip_val=1.0, default_root_dir=\"./exp1_logs/\", max_epochs=50, accelerator=\"mps\",\n",
    "                    precision='16-mixed', callbacks=[early_stop_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0c3226",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name | Type                | Params\n",
      "---------------------------------------------\n",
      "0 | ae   | SpectrumSymmetricAE | 829 K \n",
      "---------------------------------------------\n",
      "829 K     Trainable params\n",
      "0         Non-trainable params\n",
      "829 K     Total params\n",
      "3.320     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                            …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinerazocastillo/anaconda3/envs/cheminf_MS/lib/python3.11/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f846fa813c4d4fe18e494a81b69f5b58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinerazocastillo/anaconda3/envs/cheminf_MS/lib/python3.11/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(lit_AE, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3997d92b",
   "metadata": {},
   "source": [
    "# Visualize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f766c99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchview import draw_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d6eb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_graph = draw_graph(spec_AE, input_size=(64,128,2), expand_nested=True)\n",
    "model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0519f8f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
